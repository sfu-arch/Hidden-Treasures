{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ef43bb",
   "metadata": {},
   "source": [
    "# LLM Basics: Understanding Large Language Models\n",
    "\n",
    "Welcome to your first hands-on experience with Large Language Models! This notebook will guide you through the fundamental concepts and provide practical examples.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will:\n",
    "1. Understand what LLMs are and how they work\n",
    "2. Learn about model sizes and parameter counts\n",
    "3. Explore different model formats (GGUF, SafeTensors)\n",
    "4. Calculate memory requirements for different models\n",
    "5. Set up your environment for LLM work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9af9d7",
   "metadata": {},
   "source": [
    "## 1. Understanding Model Parameters\n",
    "\n",
    "Let's start by understanding what we mean by \"parameters\" in LLMs and how model size affects memory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0430ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model_memory(num_parameters, precision_bits=32):\n",
    "    \"\"\"\n",
    "    Calculate memory requirements for a model\n",
    "    \n",
    "    Args:\n",
    "        num_parameters: Number of parameters in the model\n",
    "        precision_bits: Bits per parameter (32 for float32, 16 for float16, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        Memory in bytes, MB, and GB\n",
    "    \"\"\"\n",
    "    bytes_per_param = precision_bits / 8\n",
    "    total_bytes = num_parameters * bytes_per_param\n",
    "    total_mb = total_bytes / (1024 * 1024)\n",
    "    total_gb = total_mb / 1024\n",
    "    \n",
    "    return total_bytes, total_mb, total_gb\n",
    "\n",
    "# Calculate memory for popular model sizes\n",
    "model_sizes = {\n",
    "    \"Small Model\": 1.5e9,    # 1.5B parameters\n",
    "    \"CodeLlama-7B\": 7e9,     # 7B parameters  \n",
    "    \"Llama-13B\": 13e9,       # 13B parameters\n",
    "    \"Llama-30B\": 30e9,       # 30B parameters\n",
    "    \"GPT-3\": 175e9           # 175B parameters\n",
    "}\n",
    "\n",
    "print(\"Memory Requirements for Different Model Sizes:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<15} {'Parameters':<12} {'FP32 (GB)':<10} {'FP16 (GB)':<10} {'INT8 (GB)':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, params in model_sizes.items():\n",
    "    _, _, gb_32 = calculate_model_memory(params, 32)\n",
    "    _, _, gb_16 = calculate_model_memory(params, 16)\n",
    "    _, _, gb_8 = calculate_model_memory(params, 8)\n",
    "    \n",
    "    print(f\"{name:<15} {params/1e9:>8.1f}B    {gb_32:>6.1f}     {gb_16:>6.1f}     {gb_8:>6.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59984678",
   "metadata": {},
   "source": [
    "## 2. Working with Hugging Face Models\n",
    "\n",
    "Let's explore how to download and inspect actual models from Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e25234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check system resources\n",
    "def check_system_resources():\n",
    "    \"\"\"Display current system resources\"\"\"\n",
    "    print(\"System Resources:\")\n",
    "    print(f\"CPU cores: {psutil.cpu_count()}\")\n",
    "    print(f\"RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(f\"GPU {i}: {gpu.name}\")\n",
    "            print(f\"  Memory: {gpu.memoryTotal} MB total, {gpu.memoryFree} MB free\")\n",
    "    else:\n",
    "        print(\"No CUDA GPUs available\")\n",
    "\n",
    "check_system_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e67120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and inspect a small model\n",
    "model_name = \"microsoft/DialoGPT-small\"  # Small model for demonstration\n",
    "\n",
    "print(f\"Loading tokenizer for {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Model max length: {tokenizer.model_max_length:,}\")\n",
    "\n",
    "# Test tokenization\n",
    "sample_text = \"Hello, how are you today?\"\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "print(f\"\\nSample text: '{sample_text}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Decoded: '{tokenizer.decode(tokens)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b7fb4",
   "metadata": {},
   "source": [
    "## 3. Understanding Model Architecture\n",
    "\n",
    "Let's examine the structure of a transformer model and count its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a01535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small model to examine its architecture\n",
    "print(f\"Loading model {model_name}...\")\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable and total parameters in a model\"\"\"\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return trainable_params, total_params\n",
    "\n",
    "trainable, total = count_parameters(model)\n",
    "print(f\"Trainable parameters: {trainable:,}\")\n",
    "print(f\"Total parameters: {total:,}\")\n",
    "\n",
    "# Examine model structure\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93107024",
   "metadata": {},
   "source": [
    "## 4. Model Formats and Quantization\n",
    "\n",
    "Let's understand different model formats and how quantization affects model size and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f4b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate quantization effects\n",
    "def simulate_quantization_effects():\n",
    "    \"\"\"Demonstrate the effects of quantization on model accuracy\"\"\"\n",
    "    \n",
    "    # Create sample weights (simulating a small portion of a model)\n",
    "    np.random.seed(42)\n",
    "    original_weights = np.random.normal(0, 0.1, 1000)\n",
    "    \n",
    "    # Quantize to different precisions\n",
    "    def quantize_weights(weights, bits):\n",
    "        \"\"\"Simple quantization simulation\"\"\"\n",
    "        max_val = np.max(np.abs(weights))\n",
    "        scale = (2**(bits-1) - 1) / max_val\n",
    "        quantized = np.round(weights * scale) / scale\n",
    "        return quantized\n",
    "    \n",
    "    # Test different bit precisions\n",
    "    precisions = [32, 16, 8, 4]\n",
    "    results = {}\n",
    "    \n",
    "    for bits in precisions:\n",
    "        if bits == 32:\n",
    "            quantized = original_weights  # No quantization\n",
    "        else:\n",
    "            quantized = quantize_weights(original_weights, bits)\n",
    "        \n",
    "        # Calculate error\n",
    "        mse = np.mean((original_weights - quantized)**2)\n",
    "        results[bits] = {'weights': quantized, 'mse': mse}\n",
    "    \n",
    "    # Plot results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Plot weight distributions\n",
    "    ax1.hist(original_weights, bins=50, alpha=0.7, label='Original (FP32)', density=True)\n",
    "    ax1.hist(results[8]['weights'], bins=50, alpha=0.7, label='Quantized (8-bit)', density=True)\n",
    "    ax1.set_xlabel('Weight Value')\n",
    "    ax1.set_ylabel('Density')\n",
    "    ax1.set_title('Weight Distribution: Original vs Quantized')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot quantization error\n",
    "    bits_list = list(results.keys())\n",
    "    mse_list = [results[bits]['mse'] for bits in bits_list]\n",
    "    \n",
    "    ax2.plot(bits_list, mse_list, 'bo-')\n",
    "    ax2.set_xlabel('Bits per Parameter')\n",
    "    ax2.set_ylabel('Mean Squared Error')\n",
    "    ax2.set_title('Quantization Error vs Precision')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "quantization_results = simulate_quantization_effects()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b12c23",
   "metadata": {},
   "source": [
    "## 5. Practical Exercise: Model Selection\n",
    "\n",
    "Based on what we've learned, let's create a function to help choose the right model for your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87be41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_model(available_memory_gb, task_type=\"general\"):\n",
    "    \"\"\"\n",
    "    Recommend suitable models based on available GPU memory\n",
    "    \n",
    "    Args:\n",
    "        available_memory_gb: Available GPU memory in GB\n",
    "        task_type: Type of task (\"general\", \"coding\", \"vision\")\n",
    "    \n",
    "    Returns:\n",
    "        List of recommended models\n",
    "    \"\"\"\n",
    "    \n",
    "    models = {\n",
    "        # Model name: (size_gb_fp16, capability, notes)\n",
    "        \"microsoft/DialoGPT-small\": (0.5, \"general\", \"Great for learning\"),\n",
    "        \"nvidia/Nemotron-Research-Reasoning-Qwen-1.5B\": (3, \"coding\", \"Perfect for beginners\"),\n",
    "        \"codellama/CodeLlama-7b-hf\": (14, \"coding\", \"Professional coding tasks\"),\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\": (14, \"general\", \"Efficient instruction following\"),\n",
    "        \"llava-hf/llava-v1.6-mistral-7b-hf\": (16, \"vision\", \"Vision + text understanding\")\n",
    "    }\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    print(f\"Recommendations for {available_memory_gb}GB GPU memory:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for model_name, (size, capability, notes) in models.items():\n",
    "        if size <= available_memory_gb:\n",
    "            if task_type == \"any\" or capability == task_type or capability == \"general\":\n",
    "                recommendations.append((model_name, size, capability, notes))\n",
    "    \n",
    "    if recommendations:\n",
    "        for model_name, size, capability, notes in recommendations:\n",
    "            print(f\"✅ {model_name}\")\n",
    "            print(f\"   Size: {size}GB, Capability: {capability}\")\n",
    "            print(f\"   Note: {notes}\\n\")\n",
    "    else:\n",
    "        print(\"❌ No suitable models found for your hardware configuration.\")\n",
    "        print(\"Consider using model quantization or cloud-based solutions.\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Test with different memory configurations\n",
    "for memory in [4, 8, 16, 24]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    recommend_model(memory, \"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd05a56",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "\n",
    "Congratulations! You've learned the fundamentals of LLMs. Here's what to explore next:\n",
    "\n",
    "### Immediate Next Steps:\n",
    "1. **Try the second notebook**: `02_model_inference.ipynb` - Learn to run actual model inference\n",
    "2. **Experiment with quantization**: Try loading models with different precision levels\n",
    "3. **Explore Hugging Face Hub**: Browse different models and their documentation\n",
    "\n",
    "### Advanced Topics (for later):\n",
    "1. **Fine-tuning**: Adapt models to specific tasks\n",
    "2. **LoRA**: Efficient fine-tuning techniques  \n",
    "3. **Model deployment**: Serve models in production\n",
    "4. **Custom architectures**: Build your own transformer models\n",
    "\n",
    "### Resources for Continued Learning:\n",
    "- [Hugging Face Course](https://huggingface.co/course/)\n",
    "- [PyTorch Tutorials](https://pytorch.org/tutorials/)\n",
    "- [Papers With Code](https://paperswithcode.com/area/natural-language-processing)\n",
    "\n",
    "Happy learning! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4e318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final exercise: Calculate what you could run on your system\n",
    "print(\"Your Learning Summary:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# If GPU is available, show actual specs\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory_mb = torch.cuda.get_device_properties(0).total_memory / (1024**2)\n",
    "    gpu_memory_gb = gpu_memory_mb / 1024\n",
    "    print(f\"Your GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {gpu_memory_gb:.1f} GB\")\n",
    "    \n",
    "    print(f\"\\nWhat you can run:\")\n",
    "    recommend_model(gpu_memory_gb * 0.8, \"any\")  # Use 80% of memory for safety\n",
    "else:\n",
    "    print(\"No GPU detected - you can still experiment with:\")\n",
    "    print(\"- Small models on CPU\")\n",
    "    print(\"- Cloud platforms (Google Colab, Kaggle)\")\n",
    "    print(\"- Quantized models\")\n",
    "\n",
    "print(\"\\n🎉 Congratulations on completing LLM Basics!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
